# 一、基础推理镜像列表

> 相应基础镜像已提交目标服务器！！！

## 1.1 `XPU` 基础镜像

> 基本功能: 模型推理

- `XPU`基础镜像名称: `xpu_infer_server_img:latest`
- 镜像大小: `27.2 GB`

----



## 1.2 `NPU` 基础镜像

> 基本功能: 模型推理

- `XPU`基础镜像名称: `npu_infer_server_img:latest`
- 镜像大小: `39.4 GB`

-----



## 1.3 `BPU` 基础镜像

> 基本功能: 模型推理

- `XPU`基础镜像名称: `bpu_infer_server_img:latest`
- 镜像大小: `10.9 GB`

-----



# 二、模型推理镜像`Dockerfile`

> 本地推理服务参考目录:
>
> 1. `xpu`: `/home/fz/cjh_workspace/fd_server_resource/xpu_infer_server`
> 2. `npu`(本地增删推理服务，建议使用这一版(**已支持当前所有推理设备**)): `/home/fz/cjh_workspace/fd_server_resource/real_npu_infer_server`
> 3. `bpu`: `/home/fz/cjh_workspace/fd_server_resource/bpu_infer_server`

## 2.1 `XPU Dockerfile`示例

> 已测试: `昆仑芯-R200`

```dockerfile
# 使用官方 xpu-fd 镜像作为基础镜像 -- 需修改为目标推理镜像
FROM xpu_infer_server_img:latest


# 安装系统依赖(按需下载)
#RUN apt-get update && apt-get install -y \
#    gcc \
#    g++ \
#    curl \
#    && rm -rf /var/lib/apt/lists/*

# todo: 安装cuda 或者 其他硬件的软件栈

# 设置工作目录
WORKDIR /app

# 拷贝服务目录到WORKDIR(当前目录下已包含模型文件，若不包含请自行拷贝)
COPY . .

# 安装 Python 依赖(如果需要自行取消注释)
#RUN pip install --upgrade pip && \
#    pip install -r /app/requirements.txt

# 暴露端口(与后续启动端口保持一致)
EXPOSE 8000

# 配置环境变量(需根据实际服务自行修改)
ENV MODEL_PATH=/app/package/model/model.nb \
    PARAMS_PATH="" \
    MODEL_TYPE=cls \
    MODEL_NAME=mobilenetv1 \
    DEPLOY_DEVICE=xpu \
    DEPLOY_BACKEND=fastdeploy

# 使用 uvicorn 直接启动（更适合生产环境）                                                                         
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]       
```

> 若需检查构建镜像后容器正常运行但没有得到预期结果，可将`CMD`指令更换如下，以获取服务运行时日志:
>
> `CMD uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1 > server.log 2>&1`
>
> 随后容器启动后，通过`Docker`的`exec` 指令进入运行的容器，自行查看日志分析。

-----



## 2.2 `NPU Dockerfile`示例

> 已测试: `华为-310P`

```dockerfile
# 使用官方 xpu-fd 镜像作为基础镜像 -- 需修改为目标推理镜像
FROM npu_infer_server_img:latest


# 安装系统依赖(按需下载)
#RUN apt-get update && apt-get install -y \
#    gcc \
#    g++ \
#    curl \
#    && rm -rf /var/lib/apt/lists/*

# todo: 安装cuda 或者 其他硬件的软件栈

# 设置工作目录
WORKDIR /app

# 拷贝服务目录到WORKDIR(当前目录下已包含模型文件，若不包含请自行拷贝)
COPY . .

# 安装 Python 依赖(如果需要自行取消注释)
#RUN pip install --upgrade pip && \
#    pip install -r /app/requirements.txt

# 暴露端口(与后续启动端口保持一致)
EXPOSE 8000

# 配置环境变量(需根据实际服务自行修改)
ENV MODEL_PATH=/app/package/model/model.pdmodel \
    PARAMS_PATH=/app/package/model/model.pdiparams \
    MODEL_TYPE=cls \
    MODEL_NAME=mobilenetv1 \
    DEPLOY_DEVICE=npu \
    DEPLOY_BACKEND=fastdeploy

ENV ASCEND_RT_VISIBLE_DEVICES="0"
ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64/stub:/usr/local/Ascend/ascend-toolkit/latest/acllib/lib64:/usr/local/Ascend/ascend-toolkit/latest/atc/lib64
ENV PYTHONPATH=$PYTHONPATH:/usr/local/Ascend/ascend-toolkit/latest/acllib/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/toolkit/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/atc/python/site-packages:/usr/local/Ascend/ascend-toolkit/latest/pyACL/python/site-packages/acl
ENV PATH=$PATH:/usr/local/Ascend/ascend-toolkit/latest/atc/ccec_compiler/bin:/usr/local/Ascend/ascend-toolkit/latest/acllib/bin:/usr/local/Ascend/ascend-toolkit/latest/atc/bin
ENV ASCEND_AICPU_PATH=/usr/local/Ascend/ascend-toolkit/latest
ENV ASCEND_OPP_PATH=/usr/local/Ascend/ascend-toolkit/latest/opp
ENV TOOLCHAIN_HOME=/usr/local/Ascend/ascend-toolkit/latest/toolkit


# 使用 uvicorn 直接启动（更适合生产环境）                                                                         
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]       
```

-----



## 2.3 `BPU Dockerfile`示例

>  已测试: `算能`

```dockerfile
# 使用官方 xpu-fd 镜像作为基础镜像 -- 需修改为目标推理镜像
FROM bpu_infer_server_img:latest


# 安装系统依赖(按需下载)
#RUN apt-get update && apt-get install -y \
#    gcc \
#    g++ \
#    curl \
#    && rm -rf /var/lib/apt/lists/*

# todo: 安装cuda 或者 其他硬件的软件栈

# 设置工作目录
WORKDIR /app

# 拷贝服务目录到WORKDIR(当前目录下已包含模型文件，若不包含请自行拷贝)
COPY . .

# 安装 Python 依赖(如果需要自行取消注释)
#RUN pip install --upgrade pip && \
#    pip install -r /app/requirements.txt

# 暴露端口(与后续启动端口保持一致)
EXPOSE 8000

# 配置环境变量(需根据实际服务自行修改)
ENV MODEL_PATH=/app/package/model/model.bmodel \
    PARAMS_PATH="" \
    MODEL_TYPE=cls \
    MODEL_NAME=mobilenetv1 \
    DEPLOY_DEVICE=bpu \
    DEPLOY_BACKEND=fastdeploy

# 使用 uvicorn 直接启动（更适合生产环境）                                                                         
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

-----





# 三、基于`Dockerfile`的推理镜像构建

> 以下指令需要在推理服务目录下运行:
>
> ```bash
> | - infer_server
> 	| - docs/
> 	| - package/
> 		| - fd/
> 			| - utils.py
> 			| - models.py # 模型推理接口注册
> 			| - __init__.py
> 		| - backends.py
> 		| - deploymodel.py
> 		| - ...
> 	| - test_models/
> 	| - Dockerfile
> 	| - main.py
> 	| - test_server.sh
> 	| - ...
> ```
>
> 

- `XPU`推理服务镜像建议指令如下(命名建议: `模型名_xpu_infer_img`)：

```bash
docker build -t mobilenetv1_xpu_infer_img .
```

- `XPU`推理服务镜像建议指令如下(命名建议: `模型名_npu_infer_img`)：

```bash
docker build -t mobilenetv1_npu_infer_img .
```

- `XPU`推理服务镜像建议指令如下(命名建议: `模型名_bpu_infer_img`)：

```bash
docker build -t mobilenetv1_bpu_infer_img .
```

-----



# 四、推理服务依赖项验证

> 下列步骤详细说明请查看其它相关独立文档！！！
>
> 1.`模型管理框架说明.md`
>
> 2.`后端接口说明.md`
>
> 3.`接入其它后端推理框架的说明.md`

1. 相应接口已经注册！！！

   1. 以`FastDeploy`为例，已完成`package/fd/models.py`中完成模型推理接口的注册。

2. 相应模型文件已经放置(必须以`model`文件名，存放目录: `package/model`)！！！

   1. `XPU`推理服务需要`nb`序列化模型文件: `model.nb`
   2. `NPU`推理服务需要`pdmodel`+`pdiparams`模型文件: `model.pdmodel `+ `model.pdiparams `
   3. `BPU`推理服务需要`bmodel`模型文件: `model.bmodel`

3. 推理配置文件已经放置(与模型文件存放目录一致，具体逻辑请查看`package/backends.py`的`_create_option`函数)！！！

   1. 以`FastDeploy`为例，不同模型/任务类型的推理配置`yaml`需命名为: `model.yaml`

4. 推理服务目录下`Dockerfile`文件已完成配置！！！

   1. 修改`Dockerfile`文件中当前推理服务推理基础镜像: `FROM bpu_infer_server_img:latest`

   2. 修改`Dockerfile`文件中推理服务所需的环境变量:

      ```dockerfile
      ENV MODEL_PATH=/app/package/model/model.bmodel \ # 配置模型结构文件路径(/app是镜像内推理服务所在目录)
          PARAMS_PATH="" \ # 配置模型参数文件路径(/app是镜像内推理服务所在目录)
          MODEL_TYPE=cls \ # 配置推理任务类型: cls | det | seg
          MODEL_NAME=mobilenetv1 \ # 配置推理模型的名称(需要与相应推理框架下注册的模型名一致)
          DEPLOY_DEVICE=bpu \ # 配置推理设备类型: xpu | npu | bpu
          DEPLOY_BACKEND=fastdeploy # 配置推理框架(默认值)
      ```

-----



# 五、后台推理服务容器构建

- (**后台运行**)构建的基本指令: `docker run -d --privileged -p 8000:8000 --name mobilenetv1_infer_server_test mobilenetv1_xpu_infer_img:latest`
  - 推理服务容器命名建议: `模型名_推理任务类型_推理设备烈性_infer_server`


> 镜像名请自行更换(不要遗漏`TAG`)！！！

----



# 六、测试结果示意

- `Docker` 运行状态:

```bash
57611cdb53c3   mobilenetv1_xpu_infer_img:latest                                                                                                           "uvicorn main:app --…"   5 seconds ago   Up 4 seconds                22/tcp, 0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   mobilenetv1_infer_server_test
```

- `API`测试结果:

```bash
# health接口
curl -X GET "http://localhost:8000/health"

{"status":"healthy"}

# predict接口
# file 可更换为所需本地测试路径下的图片路径
curl -X POST "http://localhost:8000/predict" \
	-F "file=@/home/fz/cjh_workspace/fd_server_resource/test.png" \
	-F "topk=5"

"{\"label_ids\": [541, 476, 577, 107, 409], \"scores\": [0.17218364775180817, 0.05998021736741066, 0.056996338069438934, 0.046304672956466675, 0.029168391600251198]}"
```

