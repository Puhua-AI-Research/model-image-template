# 一、基础镜像制作

> 已提交目标服务器！！！

- `XPU`基础镜像名称: `xpu_infer_server_img:latest`
- 镜像大小: `27.2 GB`

----

# 二、模型推理镜像制作

> 请先进入推理服务代码目录，确保当前目录下存在`Dockerfile`文件！！

- 镜像构建执行指令: `docker build -t mobilenetv1_xpu_infer_img .` 

- `Dockerfile`示例:

```dockerfile
# 使用官方 xpu-fd 镜像作为基础镜像 -- 需修改为目标推理镜像
FROM xpu_infer_server_img:latest


# 安装系统依赖(按需下载)
#RUN apt-get update && apt-get install -y \
#    gcc \
#    g++ \
#    curl \
#    && rm -rf /var/lib/apt/lists/*

# todo: 安装cuda 或者 其他硬件的软件栈

# 设置工作目录
WORKDIR /app

# 拷贝服务目录到WORKDIR(当前目录下已包含模型文件，若不包含请自行拷贝)
COPY . .

# 安装 Python 依赖(如果需要自行取消注释)
#RUN pip install --upgrade pip && \
#    pip install -r /app/requirements.txt

# 暴露端口(与后续启动端口保持一致)
EXPOSE 8000

# 配置环境变量(需根据实际服务自行修改)
ENV MODEL_PATH=/app/package/model/model.nb \
    PARAMS_PATH="" \
    MODEL_TYPE=cls \
    MODEL_NAME=mobilenetv1 \
    DEPLOY_DEVICE=xpu \
    DEPLOY_BACKEND=fastdeploy

# 使用 uvicorn 直接启动（更适合生产环境）                                                                         
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]       
```

----

# 三、基本容器构建

- (**后台运行**)构建基本指令: `docker run -d --privileged -p 8000:8000 --name mobilenetv1_infer_server_test mobilenetv1_xpu_infer_img:latest`

> 镜像名请自行更换(不要遗漏`TAG`)！！！

----

# 四、测试结果示意

- `Docker` 运行状态:

```bash
57611cdb53c3   mobilenetv1_xpu_infer_img:latest                                                                                                           "uvicorn main:app --…"   5 seconds ago   Up 4 seconds                22/tcp, 0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   mobilenetv1_infer_server_test
```

- `API`测试结果:

```bash
# health接口
curl -X GET "http://localhost:8000/health"

{"status":"healthy"}

# predict接口
curl -X POST "http://localhost:8000/predict" \
	-F "file=@/home/fz/cjh_workspace/fd_server_resource/test.png" \
	-F "topk=5"

"{\"label_ids\": [541, 476, 577, 107, 409], \"scores\": [0.17218364775180817, 0.05998021736741066, 0.056996338069438934, 0.046304672956466675, 0.029168391600251198]}"
```

