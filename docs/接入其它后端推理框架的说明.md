

# 一、构建后端维护目录

> 自定义后端: `diy`。

```bash
- package/
	- __init__.py
	- deploymodel.py
	- backends.py
	- base_option.py
	- diy/ # 自定义后端接口维护目录
		- __init__.py # 引出创建模型推理接口等的公共接口
		- models.py # 注册模型推理接口
		- utils.py # 实现创建模型推理接口的公共接口以及注册推理接口的函数
```

----

# 二、完善`__init__.py`

```python
# 引出后端的推理接口创建函数、配置项创建函数和推理结果处理函数供外部调用
from package.diy.utils import create_diy_inference_api, create_diy_option, process_diy_predict_result

# 导入后端的models完成后端推理接口的注册
import package.diy.models
```

----

# 三、完善`utils.py`

> `diy_backend`为自定义后端的本体实现。

```python
import os, sys
import json

# 如有需要可导入自定义后端的主体
import diy_backend

# 导入基本配置项，用于构建特定后端的配置项
from package.base_option import InferOption

__all__ = [
    "register_inference_api",
    "find_inference_api",
    "create_diy_inference_api",
    "create_diy_option",
    "process_diy_predict_result"
]

# 特定后端已注册接口采用字典接口进行缓存: {"cls": {"mobilenetv1": api1, "shullfulnetv2": api2}, "det": {}, "seg": {}}
DIY_INFERENCE_APIS = {}
def register_inference_api(
        model_type: str,
        model_name: str,
        infer_api
    ):
    """注册推理接口
        1. model_type用于第一级注册分类
        2. model_name用于第二级注册分类
        3. infer_api为具体的接口实现(class)
    """
    # 导入全局变量
    global DIY_INFERENCE_APIS
    if model_type not in DIY_INFERENCE_APIS.keys():
        DIY_INFERENCE_APIS[model_type] = {}
    if model_name not in DIY_INFERENCE_APIS[model_type].keys(): # 运行时注册API
        DIY_INFERENCE_APIS[model_type][model_name] = infer_api
    else:
        raise ValueError("({0} - register_inference_api) Model API({1}-{2}) has created, it should not registe repeatly.".format(
                    os.path.abspath(__file__), model_type, model_name))

def find_inference_api(
        model_type: str,
        model_name: str
    ):
    """查询已注册接口并返回接口
        1.model_type选定特定类型下的接口集合
        2.model_name选定指定模型的接口
    """
    if model_type in DIY_INFERENCE_APIS.keys() and model_name in DIY_INFERENCE_APIS[model_type].keys():
        return DIY_INFERENCE_APIS[model_type][model_name]
    else:
        raise ValueError("({0} - find_inference_api) Model API({1}-{2}) don't find.".format(
                os.path.abspath(__file__), model_type, model_name))

def create_diy_inference_api(
        model_type: str,
        model_name: str
    ):
    """创建DIY的推理接口——统一接口
    """
    _api = None
    _api = find_inference_api(model_type, model_name)
    return _api

def create_diy_option(
        model_path: str,
        params_path: str,
        config_path: str,
        deploy_device: str
    ):
    """构建DIY后端推理的配置项——统一接口
    """
    _infer_option = InferOption()
    _infer_option.name = "diy_backend"
    _infer_option.model_path = model_path
    _infer_option.params_path = params_path
    _infer_option.config_path = config_path
    _infer_option.deploy_device = deploy_device
    _infer_option.runtime_option = None
    
    # 其它配置项
    # ...

    _infer_option.check_file_is_exists()
    return _infer_option

def process_diy_predict_result(
        result,
        model_type: str,
        **kwargs
    ):
    """解析DIY后端推理结果为Json格式——统一接口
        1. 针对det检测模型结果进行阈值处理
        2. kwargs直接传入即可——默认处理threshold
    """
    _result = None
    # 1. 推理结果与kwargs参数进行匹配，比如检测目标的阈值筛选
    # 2. 获取推理结果的Json格式
    return _result

```

----

# 四、完善`models.py`

```python
import diy_backend

# 导入模型推理结构注册函数
from package.diy.utils import register_inference_api


# 分类模型推理接口注册
register_inference_api("cls", "mobilenetv1", diy_backend.xxxx_api1)

# 检测模型推理接口注册
register_inference_api("det", "yolov5", diy_backend.xxxx_api2)

# 分割模型推理接口注册
register_inference_api("seg", "unet", diy_backend.xxxx_api2)

print("register finished.")
```

> `diy_backend.xxxx_api1`接口应为接口类，提供`predict`预测接口——且接口类实例时保持一致的入口参数！！！
>
> 特别地，对于**分类模型推理接口**，`predict`应允许传入`topk`参数。

----

# 五、纳入后端管理

1. 在`backends.py`中导入新后端的服务函数:

```python
import os, sys

# 导入fastdeploy后端的推理接口创建函数、配置项创建函数以及推理结果处理函数
from package.fd import create_fd_inference_api, create_fd_option, process_fd_predict_result
from package.diy import create_diy_inference_api, create_diy_option, process_diy_predict_result
# 其余后端类似导入接口
# from package.xxxx import create_xx_inference_api, create_xx_option, process_xx_predict_result

# 已实现的后端: 添加新后端后需将其名字添入
valid_backend_list = [
    "fastdeploy",
    "diy_backend"
]
```

2. 拓展`_create_backend`函数:

```python
def _create_backend(
        model_name: str,
        model_type: str,
        deploy_backend: str
    ):
    """创建后端接口
        1. deploy_backend选择合适的后端创建接口
        2. 通过model_type和model_name创建目标推理接口
    """
    _api = None
    if deploy_backend == "fastdeploy": # 创建fastdeploy推理接口
        _api = create_fd_inference_api(
            model_type, model_name
        )
    elif deploy_backend == "diy_backend":
        _api = create_diy_inference_api(
            model_type, model_name
        )
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_backend) Backend {1} is not invalid, which only supports {2}.".format(
            os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _api
```

3. 拓展`_create_option`函数:

```python
def _create_option(
        model_path: str,
        params_path: str,
        deploy_device: str,
        deploy_backend: str,
    ):
    """创建后端接口所需的配置项
        1. model_path和params_path记录到option，以备接口实例时入口参数所需
        2. deploy_device用于特定的后端配置
        3. deploy_backend用于选择合适的后端配置项创建接口
    """
    _option = None
    if deploy_backend == "fastdeploy":
        # 从model_path自动提取config文件
        config_path = os.path.join(os.path.dirname(os.path.abspath(model_path)), "model.yaml")
        _option = create_fd_option(
            model_path, params_path, config_path, deploy_device
        )
    elif deploy_backend == "diy_backend":
        # 其它操作
        # ...
        _option = create_diy_option(
            model_path, params_path, ..., deploy_device
        )
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_option): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _option
```

4. 拓展`_create_model`含数:

```python
def _create_model(
        infer_api,
        infer_option
    ):
    """基于api和option创建模型用于推理
        1. infer_api对应目标推理接口
        2. infer_option对应目标推理的配置项
    """
    _model = None
    if infer_option.name == "fastdeploy":
        _model = infer_api(
            infer_option.model_path,
            infer_option.params_path,
            infer_option.config_path,
            runtime_option = infer_option.runtime_option
        ) # 基于配置项自主构建有效的推理接口实例
    elif deploy_backend == "diy_backend":
        _model = infer_api(
            infer_option.model_path,
            infer_option.params_path,
            ...
        )
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_model): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _model
```

5. 拓展`get_inference_result`函数:

```python
def get_inference_result(
        img,
        model,
        model_type: str,
        deploy_backend: str,
        **kwargs
    ):
    """不同后端对不同类型模型的输出结果处理方式不同，因此封装推理结果为Json格式
        1. img为opencv图像数据
        2. model对应实例化推理接口: 需总是包含predict函数，且至少包含输入图像数据的入口参数，其余参数为默认参数值
        3. model_type用于配置推理处理: cls允许传入topk，det允许传入threshold
        4. deploy_backend用于选择对应后端的推理结果处理接口
        eg:
           _result: {
                "result": {
                    Json形式的结果
                }
           }
    """
    _result = None
    predict_result = model.predict(img, 1 if "topk" not in kwargs.keys() else kwargs["topk"]) if model_type == "cls" else model.predict(img)
    if deploy_backend == "fastdeploy":
        _result = process_fd_predict_result(predict_result, model_type, **kwargs) # 通常只需要对det的检测结果作threshold过滤即可
    elif deploy_backend == "diy_backend":
        _result = process_diy_predict_result(predict_result, model_type, **kwargs) # 通常只需要对det的检测结果作threshold过滤即可
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - get_inference_result): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _result
```

