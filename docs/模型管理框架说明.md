# 一、目录结构

```bash
- package/
	- __init__.py # 引出Model供服务使用
	- deploymodel.py # 实现Model，提供load,predict接口
	- backends.py # 管理所有后端接口，提供统一的推理接口创建
	- base_option.py # 实现InferOption，管理推理时的配置项
	- fd/ # fastdeploy后端接口维护目录
		- __init__.py # 引出fastdeploy创建模型推理接口等的公共接口
		- models.py # 注册基于fastdeploy的模型推理接口
		- utils.py # 实现创建模型推理接口的公共接口以及注册推理接口的函数
	- model/  # 存放模型文件的默认路径
		- model.nb # xpu加速必须使用nb序列化模型才能绕过计算图优化等耗时操作
		- model.yaml # fastdeploy后端需要的config文件
```

----

# 二、模型推理管理(`deploymodel.py`)

> **提供统一的模型推理管理Model类，实现多种推理接口统一的调用实现**:
>
> `model = Model()`  # 创建模型推理实例——初始化调用`__init__()`实现环境变量加载
>
> `model.load()` # 加载模型
>
> `model.predict(...)` # 执行预测
>
> 注意: 加载模型依赖于`Model.__init__()`中提取的环境变量，因此需要准确设置——建议部署前进行服务测试！！！

```python
import os, sys
import time
import numpy as np
import cv2

# 导出推理接口创建的统一实现
from package.backends import create_inference_api, get_inference_result

__all__ = [
    "Model"
]

# 统一的模型推理管理
class Model:
    def __init__(self):
        """模型管理初始化工作
        1. 读取环境变量: fastdeploy所需config文件自动从_model_path同目录读取(model.yaml)
        2. 其它相关工作
        """
        self._model_path = os.getenv("MODEL_PATH", "")
        self._params_path = os.getenv("PARAMS_PATH", "")
        self._model_name = os.getenv("MODEL_NAME", "")
        self._model_type = os.getenv("MODEL_TYPE", "")
        self._deploy_device = os.getenv("DEPLOY_DIVICE", "xpu")
        self._deploy_backend = os.getenv("DEPLOY_BACKNED", "fastdeploy")

    def load(self):
        """基于初始化结果构建推理后端，并加载模型
        1. 构建后端推理接口(需封装有包含前后处理，fastdeploy后端通过model.yaml自动加载，无需设计接口)
        2. 加载模型获取推理对象
        """
        _load_start_t = time.time()
        self.model = create_inference_api(
            model_name = self._model_name,
            model_type = self._model_type,
            model_path = self._model_path,
            params_path = self._params_path,
            deploy_device = self._deploy_device,
            deploy_backend = self._deploy_backend
        )
        print("Load Model and Create API Cost Time:", (time.time()-_load_start_t)*1000., "ms")

    def predict(self, data, **kwargs):
        """模型推理接口
        1. 输入图像文件数据作为输入数据，其余参数自定义传入
        2. 解析图像文件数据为opencv-mat数据，并传入推理接口
        3. 执行推理，并反馈推理结果——固定为json格式
            eg:
                {
                    "result": {
                        根据内容自行填充
                    }
                }
        """
        img = np.frombuffer(data, np.uint8) # 图像文件buffer转通用数组
        img_data = cv2.imdecode(img, cv2.IMREAD_COLOR) # 通用数组转图像数据
        result = get_inference_result(img_data, self.model, self._model_type, self._deploy_backend, **kwargs)
        return result
```

- `create_inference_api`: 统一的推理接口创建函数——返回推理接口的实例对象，允许通过该对象的`predict`实现预测
- `get_inference_result`: 统一的推理执行函数——返回推理结果的`json`字符串



**Model类运行逻辑:**

1. 类实例化调用`__init__()`创建对象，并自动加载环境变量以供模型加载/推理接口创建所需。

```bash
# 模型管理所需的环境变量
- MODEL_PATH  		# 模型文件路径: 具体路径(如 "package/model/model.nb")
- PARAMS_PATH  		# 权重文件路径: "" 或 具体的路径(如 "package/model/model.pdiparams")
- MODEL_NAME		# 模型名称: "mobilenetv1"等
- MODEL_TYPE		# 模型类型/任务类型: 如 "cls"、 "det"、 "seg"
- DEPLOY_DIVICE		# 推理设备: 如 "xpu"、"npu"、"bpu"等
- DEPLOY_BACKNED	# 推理后端: 如 "fastdpleoy"等
```

2. 通过`load`创建模型推理接口的实例对象，该对象提供`predict`接口进行推理预测。

3. 允许调用`predict`传入`data`(前端传输的图像文件`buffer`)以及其它参数(`threshold`、`topk`)，并返回推理结果的`json`字符串。

> 因此，该类可以尽可能保证多种框架下保持不变的模型推理逻辑。

----

# 三、后端管理(`backends.py`)

> **提供统一的后端管理，实现多种后端的匹配**:
>
> `create_inference_api `: 内部依次完后端推理接口提取，配置项生成和推理接口实例化。
>
> `get_inference_result`: 提供统一的推理处理，获取`Json`格式的预测结果。

```python
def create_inference_api(
        model_name: str,
        model_type: str,
        model_path: str,
        params_path: str,
        deploy_device: str,
        deploy_backend: str,
    ):
    """创建推理接口 —— 统一推理接口
    1. 基于deploy_backend选定后端
    2. 基于model_name以及model_type选定后端已注册的推理接口
    3. 其余参数为配置项信息
    """
    _api = None
    _api = _create_backend(model_name, model_type, deploy_backend) # 获取指定后端的推理接口
    _option = None
    _option = _create_option(model_path, params_path, deploy_device, deploy_backend) # 获取指定后端推理的配置项
    _model = None
    _model = _create_model(_api, _option) # 实例化推理接口
    return _model

def get_inference_result(
        img,
        model,
        model_type: str,
        deploy_backend: str,
        **kwargs
    ):
    """不同后端对不同类型模型的输出结果处理方式不同，因此封装推理结果为Json格式
        1. img为opencv图像数据
        2. model对应实例化推理接口: 需总是包含predict函数，且至少包含输入图像数据的入口参数，其余参数为默认参数值
        3. model_type用于配置推理处理: cls允许传入topk，det允许传入threshold
        4. deploy_backend用于选择对应后端的推理结果处理接口
        eg:
           _result: {
                "result": {
                    Json形式的结果
                }
           }
    """
    _result = None
    predict_result = model.predict(img, 1 if "topk" not in kwargs.keys() else kwargs["topk"]) if model_type == "cls" else model.predict(img)
    if deploy_backend == "fastdeploy":
        _result = process_fd_predict_result(predict_result, model_type, **kwargs) # 通常只需要对det的检测结果作threshold过滤即可
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - get_inference_result): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _result
```



**后端管理逻辑:**

1. 自定义后端提供一致的推理接口查询方式，可进一步完善`_create_backend`的实现。(可拓展支持的后端)
2. 自定义后端提供一致的配置项创建方式，可进一步完善`_create_option`的实现。(完善后端运行时所需的特定参数设置)
3. 自定义后端提供一致的模型推理接口实例化方式，可进一步完善`_create_model`的实现。(实现对后端的推理拓展)

> 因此，拓展后端需要提供三种基本作用的接口函数: **接口查询、配置项创建以及接口实例化**。

----

# 四、配置项管理(`base_option.py`)

>**提供统一的配置项管理`InferOption`类，支持多种后端的运行时配置需求**:
>
>`_option = InferOption()`
>
>`_option .name = "fastdeploy" ` # eg: 针对`fastdeploy`的配置项创建
>
>`_option .model_path= "xxxx"`
>
>`......`
>
>`_option .runtime_option= "xxxx"` # `fastdeploy`后端特有的运行时配置项
>
>因此，在具体使用时，可对`InferOption`的`__init__()`实现进行拓展。

```python
import os, sys

__all__ = [
    "InferOption"
]

class InferOption:
    """推理配置项：所有后端均可引用，用于构建特定后端推理引擎的配置
        eg:
            对于fastdeploy:
                fd_option = InferOoption()
                fd_option.name = "fastdeploy"
                fd_option.model_path = "xxxxx"
                ....
    """
    def __init__(self):
        self.name = "" # 由后端创建option时指定
        self.model_path = ""
        self.params_path = ""
        self.config_path = "" # fastdpeloy时务必配置
        self.deploy_device = ""
        self.runtime_option = None # 后端运行时选项，fastdeploy务必配置

    def check_file_is_exists(self):
        """检查必要的文件是否存在——当前以Fastdeploy为例设置文件检查
        """
        if not os.path.exists(self.model_path) or self.model_path == "":
            raise ValueError("model_path is not exists or it is empty.")
        if self.model_path.split(".")[-1] == "pdmodel":
            if not os.path.exists(self.params_path) or self.params_path == "":
                raise ValueError("params_path is not exists or it is empty.")
        if self.name == "fastdeploy":
            if not os.path.exists(self.config_path) or self.config_path == "":
                raise ValueError("config_path is not exists or it is empty.")
```

- `check_file_is_exists`: 用于检查特定配置下，文件相关的运行时检查(如参数文件或配置文件是否存在——可按需拓展实现)。

----

# 五、接入基于`Fastdeploy`的后端推理

> FD后端接入的目录结构:
>
> ```bash
> - package/
> 	- __init__.py
> 	- deploymodel.py
> 	- backends.py
> 	- base_option.py
> 	- fd/ # fastdeploy后端接口维护目录
> 		- __init__.py # 引出fastdeploy创建模型推理接口等的公共接口
> 		- models.py # 注册基于fastdeploy的模型推理接口
> 		- utils.py # 实现创建模型推理接口的公共接口以及注册推理接口的函数
> ```

## 5.1 后端接入实现(`utils.py`)

1. 实现FD推理接口创建的函数: `create_fd_inference_api` —— 查询FD后端提供的服务接口并创建。

```python
def create_fd_inference_api(
        model_type: str,
        model_name: str
    ):
    """创建FD的推理接口——统一接口
    """
    _api = None
    _api = find_inference_api(model_type, model_name) # 查询已注册的FD推理接口
    return _api
```

----

2. 实现FD配置项创建的函数: `create_fd_option` —— 创建FD后端的推理配置项。

> 仅对`XPU`完成测试，其余设备`NPU(310P)`、`BPU(BM1684)`需自行验证配置合理性。

```python
def create_fd_option(
        model_path: str,
        params_path: str,
        config_path: str,
        deploy_device: str
    ):
    """构建FD后端推理的配置项——统一接口
    """
    _infer_option = InferOption()
    _infer_option.name = "fastdeploy" # 必须与后端管理判断后端类型时的命名保持一致
    _infer_option.model_path = model_path
    _infer_option.params_path = params_path
    _infer_option.config_path = config_path
    _infer_option.deploy_device = deploy_device

    _runtime_option = fd.RuntimeOption() # FD特有运行时选项
    if deploy_device == "xpu":
        _runtime_option.use_lite_backend()
        _runtime_option.use_kunlunxin()
    elif deploy_device == "npu":
        _runtime_option.use_lite_backend()
        _runtime_option.use_Ascend()
    elif deploy_device == "bpu":
        _runtime_option.use_sophgo()
    else:
        raise ValueError("({0} - create_fd_option) Deploy Device {1} is invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_device, ["xpu", "npu", "bpu"]))
    _infer_option.runtime_option = _runtime_option
    _infer_option.check_file_is_exists()
    return _infer_option
```

----

3. 实现FD模型推理过程的函数: `process_fd_predict_result` —— 处理FD后端推理过程，并提供`Json`形式的推理结果。

```python
def process_fd_predict_result(
        result,
        model_type: str,
        **kwargs
    ):
    """解析FD后端推理结果为Json格式——统一接口
        1. 针对det检测模型结果进行阈值处理
        2. kwargs直接传入即可——默认处理threshold
    """
    if model_type == "det":
        _det_result = {
            "boxes": [],
            "scores": [],
            "label_ids": [],
            #"masks": [],
            #"contrain_masks": []
        }
        threshold = kwargs["threshold"]
        for idx, score in enumerate(result.scores):
            if score >= threshold:
                _det_result["boxes"].append(result.boxes[idx])
                _det_result["scores"].append(result.scores[idx])
                _det_result["label_ids"].append(result.label_ids[idx])
                # 不支持mask, 自行匹配测试后再开启该处理
                #_det_result["masks"].append(result.masks[idx])
                #_det_result["contain_masks"].append(result.contain_masks[idx])
        return json.dumps(_det_result)
    _result = fd.vision.utils.fd_result_to_json(result) # 获取Json格式结果
    return _result
```

上述实现中，`create_fd_inference_api`和`create_fd_option`会分别被**后端管理(backends.py)**中的`_create_backend`和`_create_option`调用，以实现将FD后端接口纳入管理——并通过`_create_model`实现指定后端推理接口的实例化。

```python
# 导入fastdeploy后端的推理接口创建函数、配置项创建函数以及推理结果处理函数
from package.fd import create_fd_inference_api, create_fd_option, process_fd_predict_result
# 其余后端类似导入接口
# from package.xxxx import create_xx_inference_api, create_xx_option, process_xx_predict_result

# 已实现的后端: 添加新后端后需将其名字添入
valid_backend_list = [
    "fastdeploy"
]

def _create_backend(
        model_name: str,
        model_type: str,
        deploy_backend: str
    ):
    """创建后端接口
        1. deploy_backend选择合适的后端创建接口
        2. 通过model_type和model_name创建目标推理接口
    """
    _api = None
    if deploy_backend == "fastdeploy": # 创建fastdeploy推理接口
        _api = create_fd_inference_api(
            model_type, model_name
        )
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_backend) Backend {1} is not invalid, which only supports {2}.".format(
            os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _api

def _create_option(
        model_path: str,
        params_path: str,
        deploy_device: str,
        deploy_backend: str,
    ):
    """创建后端接口所需的配置项
        1. model_path和params_path记录到option，以备接口实例时入口参数所需
        2. deploy_device用于特定的后端配置
        3. deploy_backend用于选择合适的后端配置项创建接口
    """
    _option = None
    if deploy_backend == "fastdeploy":
        # 从model_path自动提取config文件
        config_path = os.path.join(os.path.dirname(os.path.abspath(model_path)), "model.yaml")
        _option = create_fd_option(
            model_path, params_path, config_path, deploy_device
        )
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_option): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _option

def _create_model(
        infer_api,
        infer_option
    ):
    """基于api和option创建模型用于推理
        1. infer_api对应目标推理接口
        2. infer_option对应目标推理的配置项
    """
    _model = None
    if infer_option.name == "fastdeploy":
        _model = infer_api(
            infer_option.model_path,
            infer_option.params_path,
            infer_option.config_path,
            runtime_option = infer_option.runtime_option
        ) # 基于配置项自主构建有效的推理接口实例
    #elif deploy_backend == "xxxx":
    #    pass
    else: # 不满足运行要求直接抛出异常，避免服务错误启动
        raise ValueError("({0} - _create_model): Backend {1} is not invalid, which only supports {2}.".format(
                os.path.abspath(__file__), deploy_backend, valid_backend_list))
    return _model
```

----

4. 注册FD特定模型推理接口的函数: `register_inference_api` —— 注册推理接口，以确保目标接口已注册才允许继续进行推理服务。

```python
# 特定后端已注册接口采用字典接口进行缓存: {"cls": {"mobilenetv1": api1, "shullfulnetv2": api2}, "det": {}, "seg": {}}
FD_INFERENCE_APIS = {}
def register_inference_api(
        model_type: str,
        model_name: str,
        infer_api
    ):
    """注册推理接口
        1. model_type用于第一级注册分类
        2. model_name用于第二级注册分类
        3. infer_api为具体的接口实现(class)
    """
    # 导入全局变量
    global FD_INFERENCE_APIS
    if model_type not in FD_INFERENCE_APIS.keys():
        FD_INFERENCE_APIS[model_type] = {}
    if model_name not in FD_INFERENCE_APIS[model_type].keys(): # 运行时注册API
        FD_INFERENCE_APIS[model_type][model_name] = infer_api
    else:
        raise ValueError("({0} - register_inference_api) Model API({1}-{2}) has created, it should not registe repeatly.".format(
                    os.path.abspath(__file__), model_type, model_name))
```

>因此，接入其它后端需要实现相应的服务函数: `create_xxx_inference_api`、`create_xxx_option`、`process_xxx_predict_result`、`register_inference_api`、`find_inference_api`。
>
>同时，构建如下目录结构:
>
>```bash
>- package/
>	- __init__.py
>	- deploymodel.py
>	- backends.py
>	- base_option.py
>	- xxxx/ # 后端接口维护目录
>		- __init__.py # 引出fastdeploy创建模型推理接口等的公共接口
>		- models.py # 注册基于fastdeploy的模型推理接口
>		- utils.py # 实现创建模型推理接口的公共接口以及注册推理接口的函数
>```
>
>其中，`models.py`与`utils.py`非必须，但总需要在`__init__.py`提供纳入**后端管理**所需的服务函数。

----

## 5.2 后端接口注册(`models.py`)

> 调用服务函数`register_inference_api`实现各种模型推理接口的注册，以便于`find_inference_api`进行查询——实现`create_fd_inference_api`的接口创建功能。
>
> 具体地:
>
> 注册图像分类任务的`mobilenetv1`模型推理接口实例: `register_inference_api("cls", "mobilenetv1", fd.vision.classification.PaddleClasModel)`
>
> 其中：
>
> 1. `cls`对应模型类别为图像分类，此外还有`det`、`seg`分别对应目标检测和图像分割。
>
> 2. `mobilenetv1`对应模型名称
> 3. `fd.vision.classification.PaddleClasModel`对应推理接口
>
> 因此，自定义推理接口需满足如下要求: 
>
> 1. 是一个接口类，提供`predict`预测接口——以保证后端管理中`get_inference_result`总是采用一致的预测接口。
> 2. 同一后端的推理接口须保持一致的实例化方式——以确保`_create_model`对同一后端有一致的接口实例化。

```python
import fastdeploy as fd

# 导入模型推理结构注册函数
from package.fd.utils import register_inference_api


# 分类模型推理接口注册
def register_cls_inference_api(model_list):
    """由于FD对分类模型推理接口较为统一，大部分模型通用，因此可以直接注册
    """
    for model_name in model_list:
        register_inference_api("cls", model_name, fd.vision.classification.PaddleClasModel) # 若存在无法使用通用接口实现的，则需自行构建
    return model_list

register_cls_inference_api(
    ["mobilenetv1", "shufflenetv2"]
)
# 特别的模型可以手动注册
# register_inference_api(xxxx, xxxx, xxxx)




# 检测模型推理接口注册
register_inference_api("det", "yolov5", fd.vision.detection.PaddleYOLOv5)
register_inference_api("det", "yolov8", fd.vision.detection.PaddleYOLOv8)




# 分割模型推理接口注册
def register_seg_inference_api(model_list):
    """由于FD对分割模型推理接口较为统一，大部分模型通用，因此可以直接注册
    """
    for model_name in model_list:
        register_inference_api("seg", model_name, fd.vision.segmentation.PaddleSegModel) # 若存在无法使用通用接口实现的，则需自行构建
    return model_list

register_seg_inference_api(
    ["ppliteseg", "unet"]
)

print("register finished.")

```

> 按需注册FD后端的模型推理接口——必要时需自行封装推理接口。
